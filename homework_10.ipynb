{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt1JHDsr4dHs"
   },
   "source": [
    "# <center> **CS 391, Spring 2021, Homework 10**\n",
    "### <center> Due **Tuesday, April 20, 11:59 pm ET (Boston time)**, via Gradescope\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MSiDOUbDbuv"
   },
   "source": [
    "###**Submission guidelines** \n",
    "Please write your solutions inside of this .ipynb file, then convert it to a PDF before submitting on Gradescope:\n",
    "\n",
    "*   **In Jupyter:** File > Download as > PDF\n",
    "*   **In Google Colab:** File > Print > Destination > Save as PDF\n",
    "\n",
    "When you submit, please **be sure to match the answers on your PDF to the outline on Gradescope.** In other words, if the answer to problem 2.1 is on pages 2 and 3 of your PDF, please be sure to select those pages as the answer to problem 2.1 on Gradescope. Since it takes significantly longer to grade homework that is not properly matched, **we may deduct points** for noncompliant submissions.\n",
    "\n",
    "The lab on Wednesday 1/27 covers how to get started with the notebooks for writing problem solutions and running experiments. In case you haven't done so, please sign up to the course Gradescope, with the access code **ERV7B2**: https://www.gradescope.com/courses/232562.\n",
    "\n",
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87597SVQesfG"
   },
   "source": [
    "##**Problem 1**\n",
    "\n",
    "There are n data points $$D = \\{d_1, d_2, \\ldots d_n\\}$$ in d dimensional space. Each of them has a label +1 or -1 assigned. You also have a d-dim target vector x and you would like to predict x's label by using a kNN classifier.  (k is given to you as part of the input) You will use the Euclidean distance. \n",
    "\n",
    "You don't want to spend too much time coding up your classifier. Luckily, you have an algorithm, 1NN available, that takes D and x as input  and returns the nearest neighbor  of x in D, as well as its distance $$dist(x,d_i).$$\n",
    "In notation\n",
    "$$1NN(D,x) = [i, dist(x,d_i)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "156wvZXQfe-m"
   },
   "source": [
    "**Part 1.1:**\n",
    "Describe an efficient algorithm to compute the k points nearest to x. Your algorithm should  takes 1NN as a subroutine. (You don't need to write pseudocode, but you will only get full credit if it's absolutely clear what your algorithm does.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X2IdiuyfrOL"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "We will use pseudo code to define the algorithm:\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "**Algorithm (input: k-data-point, x-vector):**\n",
    "\n",
    "    k-closest-points-to-x = array(length = k)\n",
    "    \n",
    "    $D_{copy}$ = D\n",
    "    \n",
    "    For j = 0, j < k, j++:\n",
    "    \n",
    "        resarr[i, dist(x, $d_i$)] = 1NN($D_{copy}$, x)\n",
    "        \n",
    "        k-closes-points-to-x.append(resarr)\n",
    "        \n",
    "        $D_{copy}$.nullify(resarr[0])\n",
    "        \n",
    "    endFor\n",
    "    \n",
    "    return k-closest-points-to-x\n",
    "    \n",
    "**endAlg**\n",
    "\n",
    "-----------------------------------------------\n",
    "our algroithm would return an array of tuples of all k closest data points to the vector x. Each tuple includes the index of each datapoint, as well as its distance form x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8-MoIu0ftZb"
   },
   "source": [
    "**Part 1.2:**\n",
    "Predict the label of x, where the k data points found in part 1.1 contribute to the prediction in inverse proportion of their distance $$\\frac{1}{dist(x,d_i)}.$$\n",
    "Write a formula that you can use to predict the label of x. You can keep the fraction as is in your formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUxXIN-egT_a"
   },
   "source": [
    "**Answer:**\n",
    "Where alg is the result from running the algorithm in part 1:\n",
    "\n",
    "$$x_{label} = \\frac{\\sum_{i = 0}^k{\\frac{D[alg[i][0]]_{label}}{alg[i][1]_{distance}}}}{k}$$\n",
    "\n",
    "if $x_{label} < 0$ then finalLabel = -1\n",
    "\n",
    "else if $x_{label} > 0$ then finalLabel = 1\n",
    "\n",
    "Therefore, we note that th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svCl7Os6glgn"
   },
   "source": [
    "##**Problem 2:**\n",
    "\n",
    "Let $A \\in \\mathbf{R}^{n \\times n}$ be a square matrix with eigenvalues $|\\lambda_1|\\geq |\\lambda_2|\\ldots \\geq |\\lambda_n|\\geq 0$. The principal eigenvector is called dominant if $|\\lambda_1|>|\\lambda_2|$. We learned in class, that in case $\\lambda_1$ is dominant we can use the Power Method (for sufficient many iterations) to compute the largest eigenvalue and corresponding eigenvector. That is, we can find $x$, such that $x = xA$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycutkgPNhgAs"
   },
   "source": [
    "**Part 2.1:**\n",
    "Use the power method (with 4 iterations) to compute the principal eigenvalue and corresponding eigenvector for matrix $A$ with initial vector $x^{(0)}=(1,1,1)$. Show the steps of your computation.\n",
    "$$A =  \\begin{bmatrix}\n",
    "-1&-6&0\\\\\n",
    "2&7&0\\\\\n",
    "1&2&-1\n",
    "\\end{bmatrix}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTWPJh-6hsIk"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_OCdmIKhud7"
   },
   "source": [
    "**Part 2.2:**\n",
    " Apply the power method to the matrix $B$ with initial vector  $x^{(0)}=(1,1,1)$ for four iterations. Matrix $B$ does not have a dominant eigenvalue. What do you observe during the iterations? Formulate why you cannot use the power method in this case.\n",
    "\n",
    "$$B =  \\begin{bmatrix}\n",
    "1&2&-2\\\\\n",
    "-2&5&-2\\\\\n",
    "-6&6&-3\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4urLAIYi3YQ"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZh6w7pti5e6"
   },
   "source": [
    "##**Part 3:**\n",
    "Assume the Web graph $G$ and let it consist of $n$ nodes, none of which is\n",
    "a\n",
    "sink node.  Let\n",
    "$P$ be the transition matrix of a simple random walk on this graph, and\n",
    "$P'$ the transition\n",
    "matrix of the random walk that is enhanced with a uniform jump, from any\n",
    "node to any other node in $G$, with\n",
    "probability $\\alpha$.\n",
    "That is, $P'=\\alpha P + (1-\\alpha) uv^T$, where $u$ is\n",
    "a vector of all 1's and $v$ is a uniform vector (all entries have value\n",
    "$1/n$).\n",
    "\n",
    "Show that since $P$ is a stochastic matrix, then $P'$ is also a\n",
    "  stochastic matrix. That is, show that the sum of the entries in every row\n",
    "  of $P'$ is equal to $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI0quDExjBrX"
   },
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
