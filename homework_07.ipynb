{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt1JHDsr4dHs"
   },
   "source": [
    "# <center> **CS 391, Spring 2021, Homework 7**\n",
    "### <center> Due **Tuesday, March 23, 11:59 pm ET (Boston time)**, via Gradescope\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MSiDOUbDbuv"
   },
   "source": [
    "###**Submission guidelines** \n",
    "Please write your solutions inside of this .ipynb file, then convert it to a PDF before submitting on Gradescope:\n",
    "\n",
    "*   **In Jupyter:** File > Download as > PDF\n",
    "*   **In Google Colab:** File > Print > Destination > Save as PDF\n",
    "\n",
    "When you submit, please **be sure to match the answers on your PDF to the outline on Gradescope.** In other words, if the answer to problem 2.1 is on pages 2 and 3 of your PDF, please be sure to select those pages as the answer to problem 2.1 on Gradescope. Since it takes significantly longer to grade homework that is not properly matched, **we may deduct points** for noncompliant submissions.\n",
    "\n",
    "The lab on Wednesday 1/27 covers how to get started with the notebooks for writing problem solutions and running experiments. In case you haven't done so, please sign up to the course Gradescope, with the access code **ERV7B2**: https://www.gradescope.com/courses/232562.\n",
    "\n",
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k2fY7aQ6QkS"
   },
   "source": [
    "**1. Data Analysis**\n",
    "\n",
    "This week, we will again be using the Ames Housing data set, except this time using 20 numerical features to predict sales price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "-T_jt3wMft0B",
    "outputId": "de846475-c17f-4365-b107-8826a56faac8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lot_frontage</th>\n",
       "      <th>lot_area</th>\n",
       "      <th>overall_cond</th>\n",
       "      <th>year_built</th>\n",
       "      <th>year_remod</th>\n",
       "      <th>sqft</th>\n",
       "      <th>full_bath</th>\n",
       "      <th>half_bath</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>bsmt_fin_sqft</th>\n",
       "      <th>bsmt_unfin_sqft</th>\n",
       "      <th>bsmt_full_bath</th>\n",
       "      <th>bsmt_half_bath</th>\n",
       "      <th>first_fl_sqft</th>\n",
       "      <th>second_fl_sqft</th>\n",
       "      <th>fireplaces</th>\n",
       "      <th>garage_cars</th>\n",
       "      <th>wood_deck_sqft</th>\n",
       "      <th>open_porch_sqft</th>\n",
       "      <th>sale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "      <td>2437.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>69.20</td>\n",
       "      <td>9696.03</td>\n",
       "      <td>5.57</td>\n",
       "      <td>1970.56</td>\n",
       "      <td>1984.45</td>\n",
       "      <td>1488.13</td>\n",
       "      <td>1.56</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.84</td>\n",
       "      <td>6.44</td>\n",
       "      <td>426.92</td>\n",
       "      <td>577.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1152.37</td>\n",
       "      <td>330.65</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.75</td>\n",
       "      <td>88.99</td>\n",
       "      <td>47.04</td>\n",
       "      <td>180110.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>23.34</td>\n",
       "      <td>6383.02</td>\n",
       "      <td>1.10</td>\n",
       "      <td>31.54</td>\n",
       "      <td>21.37</td>\n",
       "      <td>504.12</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.58</td>\n",
       "      <td>463.28</td>\n",
       "      <td>444.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.24</td>\n",
       "      <td>393.82</td>\n",
       "      <td>421.55</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.78</td>\n",
       "      <td>120.69</td>\n",
       "      <td>67.94</td>\n",
       "      <td>83482.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.00</td>\n",
       "      <td>1300.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1872.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>334.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>334.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12789.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>58.00</td>\n",
       "      <td>7226.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>1964.00</td>\n",
       "      <td>1124.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>229.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>866.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>127500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.00</td>\n",
       "      <td>9250.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1972.00</td>\n",
       "      <td>1994.00</td>\n",
       "      <td>1433.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>338.00</td>\n",
       "      <td>490.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1074.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>157500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.00</td>\n",
       "      <td>11207.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2003.00</td>\n",
       "      <td>2004.00</td>\n",
       "      <td>1726.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>717.00</td>\n",
       "      <td>818.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1381.00</td>\n",
       "      <td>689.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>68.00</td>\n",
       "      <td>212300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>313.00</td>\n",
       "      <td>215245.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>5642.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>5644.00</td>\n",
       "      <td>2336.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5095.00</td>\n",
       "      <td>2065.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>870.00</td>\n",
       "      <td>742.00</td>\n",
       "      <td>755000.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lot_frontage  lot_area  overall_cond  year_built  year_remod    sqft  \\\n",
       "count       2437.00   2437.00       2437.00     2437.00     2437.00 2437.00   \n",
       "mean          69.20   9696.03          5.57     1970.56     1984.45 1488.13   \n",
       "std           23.34   6383.02          1.10       31.54       21.37  504.12   \n",
       "min           21.00   1300.00          1.00     1872.00     1950.00  334.00   \n",
       "25%           58.00   7226.00          5.00     1950.00     1964.00 1124.00   \n",
       "50%           68.00   9250.00          5.00     1972.00     1994.00 1433.00   \n",
       "75%           80.00  11207.00          6.00     2003.00     2004.00 1726.00   \n",
       "max          313.00 215245.00          9.00     2010.00     2010.00 5642.00   \n",
       "\n",
       "       full_bath  half_bath  bedroom  total_rooms  bsmt_fin_sqft  \\\n",
       "count    2437.00    2437.00  2437.00      2437.00        2437.00   \n",
       "mean        1.56       0.36     2.84         6.44         426.92   \n",
       "std         0.55       0.50     0.82         1.58         463.28   \n",
       "min         0.00       0.00     0.00         2.00           0.00   \n",
       "25%         1.00       0.00     2.00         5.00           0.00   \n",
       "50%         2.00       0.00     3.00         6.00         338.00   \n",
       "75%         2.00       1.00     3.00         7.00         717.00   \n",
       "max         4.00       2.00     8.00        15.00        5644.00   \n",
       "\n",
       "       bsmt_unfin_sqft  bsmt_full_bath  bsmt_half_bath  first_fl_sqft  \\\n",
       "count          2437.00         2437.00         2437.00        2437.00   \n",
       "mean            577.82            0.42            0.06        1152.37   \n",
       "std             444.69            0.52            0.24         393.82   \n",
       "min               0.00            0.00            0.00         334.00   \n",
       "25%             229.00            0.00            0.00         866.00   \n",
       "50%             490.00            0.00            0.00        1074.00   \n",
       "75%             818.00            1.00            0.00        1381.00   \n",
       "max            2336.00            3.00            2.00        5095.00   \n",
       "\n",
       "       second_fl_sqft  fireplaces  garage_cars  wood_deck_sqft  \\\n",
       "count         2437.00     2437.00      2437.00         2437.00   \n",
       "mean           330.65        0.56         1.75           88.99   \n",
       "std            421.55        0.63         0.78          120.69   \n",
       "min              0.00        0.00         0.00            0.00   \n",
       "25%              0.00        0.00         1.00            0.00   \n",
       "50%              0.00        0.00         2.00            0.00   \n",
       "75%            689.00        1.00         2.00          168.00   \n",
       "max           2065.00        4.00         5.00          870.00   \n",
       "\n",
       "       open_porch_sqft  sale_price  \n",
       "count          2437.00     2437.00  \n",
       "mean             47.04   180110.28  \n",
       "std              67.94    83482.70  \n",
       "min               0.00    12789.00  \n",
       "25%               0.00   127500.00  \n",
       "50%              26.00   157500.00  \n",
       "75%              68.00   212300.00  \n",
       "max             742.00   755000.00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "from numpy.linalg import svd, norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "ames = pd.read_csv('https://raw.githubusercontent.com/catabia/cs391_spring21/main/AmesHousing.csv')\n",
    "ames = ames[['Lot Frontage','Lot Area', 'Overall Cond', 'Year Built', 'Year Remod/Add', 'Gr Liv Area', 'Full Bath', \n",
    "             'Half Bath', 'Bedroom AbvGr', 'TotRms AbvGrd', 'BsmtFin SF 1', 'Bsmt Unf SF', 'Bsmt Full Bath',\n",
    "             'Bsmt Half Bath', '1st Flr SF', '2nd Flr SF', 'Fireplaces', 'Garage Cars', 'Wood Deck SF',\n",
    "             'Open Porch SF','SalePrice']]\n",
    "ames.columns = ['lot_frontage','lot_area', 'overall_cond', 'year_built', 'year_remod', 'sqft', 'full_bath', \n",
    "                'half_bath', 'bedroom', 'total_rooms', 'bsmt_fin_sqft', 'bsmt_unfin_sqft', 'bsmt_full_bath',\n",
    "                'bsmt_half_bath', 'first_fl_sqft', 'second_fl_sqft', 'fireplaces', 'garage_cars', 'wood_deck_sqft',\n",
    "                'open_porch_sqft','sale_price']\n",
    "ames=ames.dropna()\n",
    "ames.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwqeSXxh52Cx"
   },
   "source": [
    "Here is a simple method for figuring out how long it takes snippet of code to run in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN8Z_Fj65XAS",
    "outputId": "8c9e5775-1554-4e1f-a78c-d74babcc8316",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello word!\n",
      "It took 0.0001537799835205078  seconds to print \"Hello world!\" once.  Pretty speedy, eh?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print('Hello word!')\n",
    "end = time.time()\n",
    "print('It took', end-start, ' seconds to print \"Hello world!\" once.  Pretty speedy, eh?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtrkMw6lcD0G"
   },
   "source": [
    "**1.1** Normalize all features in the dataset to be between 0 and 1.  (This isn't necessary for OLS regression, but since we're predicting home sale prices, which tend to be very large, normalizing will keep our numbers small and easy to read).  Then, set up your $X$ and $y$ so that you may predict sale price using all other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Cgb_plztcEHx"
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#start by normalizing the data:\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "x = scaler.fit_transform(ames.drop(['sale_price'], axis=1))\n",
    "y = scaler.fit_transform(ames.sale_price.values.reshape(ames.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M44N9sJsbJxB"
   },
   "source": [
    "**1.2**\n",
    "\n",
    "In Lab 7, we learned how to analytically solve a linear regression using the normal equation.  The function LinearRegression from SciKit Learn (which we used on Homework 6) uses this technique.  Please compute the linear regression using this method, as well as the MSE of the model. How long does it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vQb8XLky7CDD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all, the time it took us to compute the linear regression and MSE is  0.014699697494506836  seconds.\n",
      "The MSE was:  0.0027658972465704162\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start = time.time()\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "prediction = model.predict(x)\n",
    "mse_a = mean_squared_error(prediction, y)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Over all, the time it took us to compute the linear regression and MSE is \", end-start, \" seconds.\")\n",
    "print(\"The MSE was: \", mse_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvDbmCpzduSf"
   },
   "source": [
    "**1.3**\n",
    "\n",
    "In Lab 8, we learned how to solve a linear regression using gradient descent.  The SciKit Learn function SGDRegressor also uses gradient descent.  Now, solve your linear regression using this method, with a learning rate of 0.0001 and a total of 100,000 iterations.  What is the MSE of this model?  How long does it take?  Is it significantly faster or slower than the normal equation method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qUCpzMyDYEJ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over all, the time it took us to compute the MSE of the model is  0.006687164306640625  seconds.\n",
      "The MSE was:  0.004122761848742904\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "start = time.time()\n",
    "model = SGDRegressor(alpha = 0.0001, max_iter = 100000)\n",
    "y=y.reshape(y.shape[0],)\n",
    "model.fit(x, y)\n",
    "mse_b = ((model.predict(X) - y)**2).mean()\n",
    "end = time.time()\n",
    "print(\"Over all, the time it took us to compute the MSE of the model is \", end-start, \" seconds.\")\n",
    "\n",
    "print(\"The MSE was: \", mse_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rO9ZP-jiwrC"
   },
   "source": [
    "**Answer:**\n",
    "We can see that using the linear regression method took us less time than the gradient-descent method (~40% of the time) to calculate the MSe, which is significantly less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFq9q1Qkg-zg"
   },
   "source": [
    "**1.4**\n",
    "\n",
    "Of the two linear regression models you computed in **2.2** and **2.3**, which one has the lowest MSE?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z62HJowPhYiw"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "We notice that section **1.2** (the first time we computed the MSE) resulted in lower MSE (0027658972465704162) and **1.3** (where we used the gradient descent method) had a greater MSE (0.004122761848742904)\n",
    "\n",
    "That's due to the number of iterations and learning rate perfromed when computing the MSE, Since the learning rate is fixed, and the number of the iterations could possibly be bigger (as it is probably in **1.2**), then our MSE is further away from 0 when using the gradient descent method - implying the model's greater imperfection in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KllbKtSBeqZV"
   },
   "source": [
    "**1.5**\n",
    "\n",
    "In Lab 7 and HW 6, we learned that we could solve a polynomial regression by fitting a linear regression to polynomial features (using SciKit learn's PolynomialFeatures method makes this simple).  Create cubic features (degree=3) for your $X$ data.  How many feature columns do you now have? Explain why there are so many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WB8fGEpYY83j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "model = PolynomialFeatures(3) \n",
    "x_degree3 = model.fit_transform(x)\n",
    "model = LinearRegression()\n",
    "model.fit(x_degree3, y)\n",
    "prediction = model.predict(x_degree3)\n",
    "\n",
    "print(len(x_degree3[0]))\n",
    "\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgyhrmFriI4n"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "We see that we have 1771 columns now in the 3rd degree polynomial regression of the provided dataset (for our X data).\n",
    "Thet's due to the fact that we use a 3rd degree polynomial to approximate a 20 column matrix, or in other words, we take into account 20 categories and aspects of the dataset and use a regression (a 3rd degree polynomial regression) to approximate certain values based on that provided data - which has 20 (which is a lot) columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDC3kjgGiOpT"
   },
   "source": [
    "**1.6** Now, using both the normal equation (LinearResgression) and gradient descent (SGDRegressor) methods, compute a cubic regression of your data. Which method is faster?  Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-fhYdMKcapGf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1 (Polynomial regression) took us :  2.869316816329956  seconds.\n",
      "Method 2 (Gradient Descent) took us :  0.010333061218261719  seconds.\n"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "degree = 3\n",
    "# method 1 : Polynomial Regression\n",
    "start1 = time.time()\n",
    "model = PolynomialFeatures(degree) \n",
    "x_degree3 = model.fit_transform(x)\n",
    "model = LinearRegression()\n",
    "model.fit(x_degree3, y)\n",
    "prediction = model.predict(x_degree3)\n",
    "end1 = time.time()\n",
    "\n",
    "# method 2 : Gradient Descent - CURRENTLY NOT SURE ABOUT\n",
    "start2 = time.time()\n",
    "model = SGDRegressor(alpha = 0.0001, max_iter = 100000)\n",
    "y=y.reshape(y.shape[0],)\n",
    "model.fit(x, y)\n",
    "prediction = ((model.predict(X) - y)**degree)\n",
    "end2 = time.time()\n",
    "\n",
    "\n",
    "#summary:\n",
    "print(\"Method 1 (Polynomial regression) took us : \", end1 - start1, \" seconds.\")\n",
    "print(\"Method 2 (Gradient Descent) took us : \", end2 - start2, \" seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku1MUIFAiC9i"
   },
   "source": [
    "**Answer:**\n",
    "For the third degree we see that the gradient descent method works significantly quicker.\n",
    "Polynomial regression only works better than graident descent for a 2nd degree prediction (which is simply just a linear regression). That's simply due to the fact that creating such a large matrix for any degree larger than 2 takes much more time to traverse and calculate the necessary computation, where as in gradient descent the computation method is relied upon the use forumla, and therefore for any computation degree larger than 2 (3 or above), usng the gradient descent method is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "homework7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
